{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zriTdjauH8iQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmTTyjUGqol"
      },
      "source": [
        "### Build-a-transformer\n",
        "\n",
        "In this section, you will implement a transformer language model layer by layer, then use it to generate (hopefully) coherent text.\n",
        "\n",
        "To understand how these layers work, please check out our guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
        "\n",
        "\n",
        "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) - a prominent model from 2019.\n",
        "\n",
        "\n",
        "\n",
        "Idea & code by: Ilya Beletsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOcK0lGTGqol",
        "outputId": "a60f4863-4114-4647-ec6e-464e6c7103d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
        "for key, value in tuple(state_dict.items()):\n",
        "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
        "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
        "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
        "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
        "\n",
        "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0SUtQnGqom"
      },
      "source": [
        "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
        "\n",
        "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
        "\n",
        "The fully connected layers are by far easier to understand, so we shall begin there:\n",
        "\n",
        "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3Rh-6DX9Gqom"
      },
      "outputs": [],
      "source": [
        "class GeLUThatWasUsedInGPT2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
        "        self.gelu = GeLUThatWasUsedInGPT2()\n",
        "        self.c_proj = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVGKnHBGqom"
      },
      "source": [
        "Now, let's test that it works with GPT-2 weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CoWjZwZkGqom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33fa94b-9a61-401b-bee6-80287af6cde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems legit!\n"
          ]
        }
      ],
      "source": [
        "mlp = FullyConnected(dim=768)\n",
        "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
        "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
        "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
        "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(mlp(x) * x)\n",
        "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
        "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
        "print(\"Seems legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbfCevRwGqom"
      },
      "source": [
        "Now, let's get to attention layers.\n",
        "\n",
        "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
        "\n",
        "As before, please implement masked self-attention __without layernorm or residual connections.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T6j7M4hLGqon"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "        bs, sl = q.shape[0], q.shape[1]\n",
        "        n_h, h_s = self.num_heads, self.head_size\n",
        "\n",
        "        q = q.view(bs, sl, n_h, h_s).transpose(1, 2)\n",
        "        k = k.view(bs, sl, n_h, h_s).transpose(1, 2)\n",
        "        v = v.view(bs, sl, n_h, h_s).transpose(1, 2)\n",
        "\n",
        "        scaling_factor = (h_s ** 0.5)\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / scaling_factor\n",
        "\n",
        "        causal_mask = torch.tril(torch.ones((sl, sl), device=x.device, dtype=torch.bool))\n",
        "        causal_mask = causal_mask.view(1, 1, sl, sl).repeat(bs, n_h, 1, 1)\n",
        "\n",
        "        attention_scores = attention_scores.masked_fill(~causal_mask, torch.finfo(attention_scores.dtype).min)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous()\n",
        "        context = context.reshape(bs, sl, n_h * h_s)\n",
        "\n",
        "        return self.c_proj(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umZpcpIkJva7"
      },
      "source": [
        "Test that it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg5Oj_PPM6hj",
        "outputId": "44b8fce1-287c-4776-9b94-8ee0e1ac3ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It works!\n"
          ]
        }
      ],
      "source": [
        "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
        "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
        "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
        "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
        "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(attn(x) * x)\n",
        "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
        "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
        "print(\"It works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn6tgTHzOK4l"
      },
      "source": [
        "We can now combine attention and MLP to build the full transformer layer:\n",
        "\n",
        "![img](https://i.imgur.com/1sq2vHO.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p3AH7YQvRpvU"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FullyConnected(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res1 = self.ln_1(x)\n",
        "        res1 = self.attn(res1)\n",
        "        res1 = res1 + x\n",
        "        res2 = self.ln_2(res1)\n",
        "        res2 = self.mlp(res2)\n",
        "        return res2 + res1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzo_QeFVSNZa",
        "outputId": "cd1fe892-0bac-48db-bd28-40b55018b70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "layer = TransformerLayer(dim=768, num_heads=12)\n",
        "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
        "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mbqw9iuaSrYy"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
        "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
        "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
        "\n",
        "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        token_embeddings = self.wte(input_ids)\n",
        "        position_embeddings = self.wpe(position_ids)\n",
        "        full_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "        transformer_output = self.h(full_embeddings)\n",
        "        transformer_output_ln = self.ln_f(transformer_output)\n",
        "\n",
        "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
        "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
        "        return output_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p0m8jt66aDIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbc7e9f-b36a-45e6-a96d-95903efe6dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  look\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
        "\n",
        "predicted_logits = model(input_ids)\n",
        "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
        "\n",
        "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ql3Lo7dXZ2",
        "outputId": "674fa0b1-aa8c-408d-e7b3-0b4dbd246655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Fermi paradox  contains two (following the Fermi Paradox  5) main and do not reflect common scaling patterns.  \n",
            "Making\n",
            " of (Che)an\n",
            "In 1454 the Emperor Francis sailed back to Spain (without Comte) and once again Joey.ca did necessary work that\n",
            " led back into Paris (after Diego and Catrici's time). Apparently, Catrici was Bordeaux's king, who ordered a column of thieves\n",
            " to steal his possessions and flee to Africa. Zulu Italy and Bahia at the border allowed authorities to arrest him and arrested\n",
            " him again. He was brutally tortured as well. In  to in late 1511 a group of Aborigines set sail from Wild Marsch Vinaugh\n",
            "   aptly known as \"the one and only\" Archbishops of Bandaecsur in what was then the shadow of Mozambique.\n",
            "This was the external\n",
            "  door being opened to the bigger world. Or perhaps its high-priest knew \"Star Cup\", up their chickens until the Thames Cancer\n",
            " Trust in a  hatchet. Some might say they were princes, others would be liberals, the rest eventually would accept the challenge\n",
            " of spiritual selfenforcement and reform and follow rules without force toward the edge of these totalitarian arms.   There\n",
            " don't seem to be enough small socialists for any picture of Julia and Leo and Mickey owning the supermarket Fiffel Tower\n",
            ".  That was the way Geist closed down Tibet (just look at the Duck-Creep GeForce polo!) or that triggered the Cherokee in\n",
            " Permit investigations. Now Italy officials take responsibility for all this suffering womenfolk eat and grave caribou logging\n",
            " on the ocean floor. Staggering more and more measures were put in place over the fashions factory and its zombies from the\n",
            " 1960s and 70s.  The Equal Rights Movement ended up being an interesting feature of Italian fascism.\n",
            "More Ancient Gods: Mary\n",
            " of Dutchess - Apollo, Arcadia, Jack of all Trades arr. Day of Goddess breasts - Italians dream of writing and weaving Bell\n",
            "ars and whaling relics. Raising Doll free dancing of kids plus film practice. The Lily of Giza was a native of Sweden. Fighting\n",
            " their way into immortal Heaven AND into Virginia on a youthful Super Pony emerged in 1395 Rabbi Theo Kubon BELONGED TO FR\n",
            "ANGIS RUTZ:\n",
            "Columbus conquered the Sagame\n",
            "Noah conceived of either sides Figuresllan or Wez"
          ]
        }
      ],
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens]))\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3NJ0ocgGqop"
      },
      "source": [
        "__Reminder:__ after class, please go to `MaskedSelfAttention.forward` above and finish the job!\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Here's how you can do the same with transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NTOHu124Gqop",
        "outputId": "497d0ca2-fdf3-4f6f-e804-c90fe533c17d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:  The Fermi paradox  is perhaps the least interesting and least understood phenomena of the Universe .  Because it is a very small world, it cannot be considered a complete vacuum, and so it is not a vacuum we could find with some simple telescopes.  It\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print('Generated text:', tokenizer.decode(\n",
        "    model.generate(\n",
        "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
        "        do_sample=True, max_new_tokens=50\n",
        "    ).flatten().numpy()\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}